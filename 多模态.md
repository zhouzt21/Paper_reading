多模态训练基础文献阅读

## ViT （vision transformer）

https://zhuanlan.zhihu.com/p/445122996

#### An image is worth 16x16 words: Transformers for image recognition at scale（ViT）

ViT是2020年Google团队提出的将Transformer应用在图像分类的模型，虽然不是第一篇将transformer应用在视觉任务的论文，但是因为其模型“简单”且效果好，可扩展性强（scalable，模型越大效果越好），成为了transformer在CV领域应用的里程碑著作，也引爆了后续相关研究

把最重要的说在最前面，ViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果

但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。CNN具有两种归纳偏置，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）， f(g(x))=g(f(x)) ，其中g代表卷积操作，f代表平移操作。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型

结构：

ViT将输入图片分为多个patch（16x16），再将每个patch投影为固定长度的向量送入Transformer，后续encoder的操作和原始Transformer中完全相同。但是因为对图片分类，因此在输入序列中加入一个特殊的token，该token对应的输出即为最后的类别预测

(1) patch embedding：通过patch embedding将一个视觉问题转化为了一个seq2seq问题

(2) positional encoding

(3) LN/multi-head attention/LN



MLP-Mixer: An all-MLP Architecture for Vision （用MLPs替代self-attention可以得到和ViT同样好的结果）

---

### 多模态预训练CLIP

CLIP（Contrastive Language–Image Pre-training）[1]是OpenAI的第一篇多模态预训练的算法，它延续了GPT系列“大力出奇迹”的传统。模型是一个基于图像和文本并行的多模态模型，然后通过两个分支的特征向量的相似度计算来构建训练目标。为了训练这个模型，OpenAI采集了超过4亿的图像-文本对。CLIP在诸多多模态任务上取得了非常好的效果，例如图像检索，地理定位，视频动作识别等等，而且在很多任务上仅仅通过无监督学习就可以得到和主流的有监督算法接近的效果。CLIP的思想非常简单，但它仅仅通过如此简单的算法也达到了非常好的效果，这也证明了多模态模型强大的发展潜力。

1. 数据集

* 对比开放的计算机视觉应用，目前的所有的视觉公开数据集（例如ImageNet等）的应用场景都是非常有限的，为了学习到通用的图像-文本多模态通用特征，我们首先要做的便是采集足够覆盖开放计算机视觉领域的数据集。

* 这里OpenAI采集了一个总量超过4亿图像-文本对的数据集WIT（WebImage Text）。为了尽可能的提高数据集在不同场景下的覆盖度，WIT的首先使用在英文维基百科中出现了超过100次的单词构建了50万个查询，并且使用WordNet进行了近义词的替换。

2. 算法

CLIP的核心思想是将图像和文本映射到同一个特征空间。这个特征空间是一个抽象的概念，例如当我们看到一条狗的图片的时候，我们心中想的是狗，当我们读到狗的时候我们想的也是狗，那么我们心中想象的狗，便是“特征空间”。

CLIP也是由两个编码器组成：

- 图像编码器：用于将图像映射到特征空间；CLIP的图像编码器选择了5个不同尺寸的残差网络[2]以及3个不同尺寸的ViT[3]，并对模型细节做了调整.

- 文本编码器：用于将文本映射到相同的特征空间。CLIP的文本编码器使用的是Transformer[5]

在模型训练过程中，我们取到的每个batch由 N 个图像-文本对组成。这 N个图像送入到图像编码器中会得到N 个图像特征向量 (I1,I2,⋯,IN) ，同理将这 N 个文本送入到文本编码器中我们可以得到 N 个文本特征向量 (T1,她T2,⋯,TN) 。因为只有在对角线上的图像和文本是一对，所以CLIP的训练目标是让是一个图像-文本对的特征向量相似度尽可能高，而不是一对的相似度尽可能低，这里相似度的计算使用的是向量内积。通过这个方式，CLIP构建了一个由 N个正样本和 N^2−N个负样本组成的损失函数。

3. 总结

CLIP是一个OpenAI特色非常重的文章，一是表现在它采集了大量的数据以及使用了大量的训练资源，二是不是非常侧重算法上的创新。这充分体现了目前搞预训练正逐步成为企业垄断的方向，表现在无论是数据，还是计算资源，都是个人和小机构无法承担的。CLIP的技术突破不大，但是效果非常惊艳，作为多模态预训练的算法之一，充分证明了这个方向庞大的科研潜力，起到了抛砖引玉的作用。

-----

### ViLD

今年年初OpenAI的CLIP将图像和文本通过对比学习的方法联系起来，并且Zero-Shot的效果堪比ResNet50。

最近几天Google Research刚出的ViLD将CLIP应用到了检测任务上，在新增类别上Zero-Shot超过Supervised的方法。难道Zero-Shot又要开始了。

https://zhuanlan.zhihu.com/p/369464298